
#########
# Kafka #
#########

###
spark.kafka.bootstrap.servers=localhost:9092
###
spark.kafka.metadata.broker.list=localhost:9092
###
spark.kafka.topics=eci

#########
# Spark #
#########

###
spark.streaming.seconds=5
###
spark.streaming.checkpoint.dir=checkpoints-DATE
###
# Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5).
# This enables the Spark Streaming to control the receiving rate based on the current batch scheduling
# delays and processing times so that the system receives only as fast as the system can process.
# Internally, this dynamically sets the maximum receiving rate of receivers.
# This rate is upper bounded by the values spark.streaming.receiver.maxRate and
# spark.streaming.kafka.maxRatePerPartition if they are set (see below).
# default: false
###
spark.streaming.backpressure.enabled=true
###
# Maximum rate (number of records per second) at which data will be read from each Kafka partition
# when using the new Kafka direct stream API. See the Kafka Integration guide for more details.
# default: not set
###
spark.streaming.kafka.maxRatePerPartition=1
###
# Maximum number of consecutive retries the driver will make in order to find the latest offsets on the
# leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts).
# Only applies to the new Kafka direct stream API.
# default: 1
###
#spark.streaming.kafka.maxRetries=1


#################
# Elasticsearch #
#################

###
spark.es.resource=eci-model-test/product
###
# Size (in bytes) for batch writes using Elasticsearch bulk API.
# Note the bulk size is allocated per task instance.
# Always multiply by the number of tasks within a Hadoop job to get the total bulk size at runtime hitting ES.
# default: 1mb
###
#spark.es.batch.size.bytes=1MB
###
# Size (in entries) for batch writes using Elasticsearch bulk API - (0 disables it).
# Companion to es.batch.size.bytes, once one matches, the batch update is executed.
# Similar to the size, this setting is per task instance; it gets multiplied at runtime by the total number
# of Hadoop tasks running.
# default: 1000
###
es.batch.size.entries=1
###
# Whether to invoke an index refresh or not after a bulk update has been completed.
# Note this is called only after the entire write (meaning multiple bulk updates) have been executed.
# default: true
###
#es.batch.write.refresh=true
###
# Number of retries for a given batch in case Elasticsearch is overloaded and data is rejected.
# Note that only the rejected data is retried. If there is still data rejected after the retries have been
# performed, the Hadoop job is cancelled (and fails). A negative value indicates infinite retries;
# be careful in setting this value as it can have unwanted side effects.
# default: 3
###
#es.batch.write.retry.count=3
###
# Time to wait between batch write retries.
# default: 10s
#es.batch.write.retry.wait=10s



